## FunctionDef get_embedding(text)
**get_embedding**: The function of get_embedding is to generate an embedding vector for a given text input.

**parameters**: The parameters of this Function.
· text: A string input representing the text for which the embedding vector is to be generated.

**Code Description**: The get_embedding function utilizes the OpenAIEmbeddings class to create an embedding vector from the provided text. When invoked, it first initializes an instance of OpenAIEmbeddings and then calls the embed_query method with the input text. This method processes the text and returns a numerical representation (embedding) that captures the semantic meaning of the text. 

The get_embedding function is called within the invoke_rag_with_repo function, which is responsible for processing user input in the context of an agent that may rely on a repository of resources. When the agent receives input, it checks if it has an associated repository. If so, it calls get_embedding to transform the input text into an embedding vector. This vector is then used to search for similar resources in the repository using the pgVectorTools.search_similar_resources method. The results of this search are subsequently processed and formatted for output.

This relationship highlights the role of get_embedding as a critical step in enabling the agent to find relevant information based on user input, thereby enhancing the overall functionality of the system.

**Note**: It is important to ensure that the input text is properly formatted and relevant to the context in which the embeddings are being generated, as this will impact the quality of the resulting embeddings.

**Output Example**: An example of the output from get_embedding might be a list or array of floating-point numbers, such as: 
[0.1234, -0.5678, 0.9101, ...] 
This output represents the embedding vector corresponding to the input text.
## FunctionDef invoke(agent, input)
**invoke**: The function of invoke is to process input through a specified agent's language model and return the generated output.

**parameters**: The parameters of this Function.
· agent: An object representing the agent, which contains the model configuration and prompts.
· input: The input data that will be processed by the agent's language model.

**Code Description**: The invoke function is designed to facilitate interaction with a language model associated with a given agent. It begins by printing the name of the agent, which helps in tracking the source of the invocation during execution. The function then creates instances of SystemMessage and HumanMessage using the agent's system prompt and prompt template, respectively. 

Next, it constructs a ChatPromptTemplate using the system and human messages, which formats the input for the language model. The output parser is instantiated as StrOutputParser, which is responsible for processing the output generated by the model. The function retrieves the appropriate language model by calling the getLLM function, passing the agent as an argument. This ensures that the model used is aligned with the agent's configuration.

The core of the invoke function is the creation of a processing chain, which combines a RunnablePassthrough, the prompt, the model, and the output parser. This chain is designed to sequentially process the input through each component, ultimately generating a response based on the input provided.

The invoke function is called by other functions, such as invoke_rag_with_repo. In invoke_rag_with_repo, if the agent has no repository, it directly calls invoke to handle the input processing. This illustrates the role of invoke as a fundamental processing unit within the module, allowing for flexible input handling based on the agent's capabilities.

**Note**: It is crucial to ensure that the agent passed to the invoke function is properly configured with a valid model and prompts. If the agent lacks a model or the model is not recognized, the function may not perform as intended.

**Output Example**: If the agent is configured with a model that generates a response based on the input "What is the weather today?", the function might return a string such as "The weather today is sunny with a high of 75 degrees."
## FunctionDef invoke_rag_with_repo(agent, input)
**invoke_rag_with_repo**: The function of invoke_rag_with_repo is to process user input through an agent that utilizes a repository of resources, retrieving relevant information based on the input.

**parameters**: The parameters of this Function.
· agent: An instance of the Agent class, which contains the configuration and prompts for the agent.
· input: The input data that will be processed to retrieve relevant information.

**Code Description**: The invoke_rag_with_repo function begins by checking if the provided agent has an associated repository. If the agent's repository is None, it prints a message indicating that the agent cannot rely on any repository and directly invokes the invoke function to process the input. This demonstrates a fallback mechanism where, in the absence of a repository, the function still attempts to handle the input through the agent's language model.

If the agent does have a repository, the function proceeds to print the agent's name for tracking purposes. It then calls the get_embedding function to generate an embedding vector from the input text. This embedding is crucial for the subsequent search for similar resources within the agent's repository, which is performed using the pgVectorTools.search_similar_resources method. The search is limited to retrieving one result, as specified by the RESULTS parameter.

The function initializes an empty string, info, to accumulate the content of the similar resources found. It iterates over the results returned from the search, appending the page content of each result to the info string. This content is formatted to provide clarity on the information retrieved.

Next, the function constructs a ChatPromptTemplate using the agent's system prompt and the accumulated information, along with the agent's prompt template. This template is designed to format the input for the language model effectively.

An output parser, StrOutputParser, is instantiated to process the output generated by the language model. The appropriate language model is retrieved by calling the getLLM function, which ensures that the model aligns with the agent's configuration.

Finally, the function creates a processing chain that combines a RunnablePassthrough, the prompt, the model, and the output parser. This chain is executed with the input, and the resulting output is returned. The invoke_rag_with_repo function thus serves as a bridge between user input and the agent's capabilities, leveraging both the repository and the language model to provide relevant responses.

**Note**: It is essential to ensure that the agent passed to invoke_rag_with_repo is properly configured with a valid repository and model. If the agent lacks a repository, the function will revert to using the invoke function, which may not provide the same level of contextual information retrieval.

**Output Example**: A possible output from invoke_rag_with_repo could be a string containing relevant information retrieved from the repository, such as: "INFO CHUNK: The latest research on AI indicates significant advancements in natural language processing. Source: Research Journal, page: 12."
## FunctionDef invoke_ConversationalRetrievalChain(agent, input, session)
**invoke_ConversationalRetrievalChain**: The function of invoke_ConversationalRetrievalChain is to facilitate a conversational retrieval process by utilizing a language model and a memory structure to respond to user queries based on chat history and context.

**parameters**: The parameters of this Function.
· agent: An object representing the agent, which contains the model configuration and identification.
· input: The user input or query that needs to be processed and responded to.
· session: A dictionary that maintains the session state, including memory and application identifiers.

**Code Description**: The invoke_ConversationalRetrievalChain function is designed to manage a conversational retrieval process by leveraging a language model and a memory structure. It begins by printing the application ID from the session to track the context of the operation. The function constructs a unique memory key based on the agent's ID and checks if this key exists in the session. If the key is not present, it initializes a new instance of ConversationBufferMemory, which is used to store the chat history and facilitate context-aware responses.

Next, the function retrieves the appropriate language model by calling the getLLM function, passing the agent as an argument. This model will be used to generate responses based on the input provided by the user. The function then obtains a retriever instance from pgVectorTools, which is responsible for fetching relevant documents or context based on the agent's repository ID.

A prompt template is defined, which structures the input for the language model. This template includes placeholders for chat history, context, and the new question, ensuring that the model receives all necessary information to generate a coherent response.

The function then creates a ConversationalRetrievalChain using the language model, retriever, and memory instance. This chain is configured to not return source documents and is set to verbose mode for detailed logging. The custom chain is invoked with the user input, and the result is printed for debugging purposes.

Finally, the function returns the answer extracted from the result, which is expected to be a structured response generated by the language model based on the provided context and chat history.

This function is called within the context of conversational interactions, specifically designed to enhance user engagement by providing contextually relevant answers. It relies on the getLLM function to ensure that the correct language model is utilized, thereby maintaining consistency and accuracy in responses.

**Note**: It is essential to ensure that the session dictionary is properly maintained and that the agent passed to the function has a valid configuration. The memory structure must be initialized correctly to store and retrieve chat history effectively.

**Output Example**: A possible return value of the function could be a dictionary containing the answer to the user's query, such as:
```json
{
  "answer": "Based on the previous conversation and the context provided, the answer to your question is..."
}
```
## FunctionDef getLLM(agent)
**getLLM**: The function of getLLM is to retrieve the appropriate language model based on the agent's configuration.

**parameters**: The parameters of this Function.
· agent: An object representing the agent, which contains the model configuration.

**Code Description**: The getLLM function is designed to return a specific language model instance based on the properties of the provided agent. It first checks if the agent's model attribute is None; if it is, the function returns None, indicating that there is no model available for the agent. If the model is present, the function evaluates the provider of the model. If the provider is "OpenAI", it instantiates and returns a ChatOpenAI object using the model's name. Conversely, if the provider is "Anthropic", it creates and returns a ChatAnthropic object with the same model name. If the model's provider does not match either of these known types, the function defaults to returning None.

The getLLM function is called by several other functions within the same module, specifically invoke, invoke_rag_with_repo, and invoke_ConversationalRetrievalChain. Each of these functions relies on getLLM to obtain the appropriate language model before proceeding with their respective operations. For instance, in the invoke function, getLLM is called to retrieve the model, which is then used in a chain of operations that includes processing user input and generating responses. Similarly, invoke_rag_with_repo and invoke_ConversationalRetrievalChain utilize getLLM to ensure they are working with the correct model for their tasks, which involve more complex interactions with user input and retrieval of information.

**Note**: It is important to ensure that the agent passed to getLLM has a properly configured model. If the model is None or the provider is unrecognized, the function will not be able to return a valid language model instance.

**Output Example**: If the agent has a model with provider "OpenAI" and name "gpt-3.5-turbo", the function would return an instance of ChatOpenAI initialized with "gpt-3.5-turbo". If the agent's model is None, the function would return None.
